{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1uwMeaVJ_aSWPY__MvnhJsQUHU77QYdnz","authorship_tag":"ABX9TyMJFXHi/yibOepalTtEDLA6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5849aea4f9b640349f56d43437813166":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_757442692f374d83b50fd1c65464aa80","IPY_MODEL_af35a654bf9244bbaef897af30bd1314","IPY_MODEL_68cbe962fc3e4ea88756cc9d339cabe2"],"layout":"IPY_MODEL_0ef59309dccb46cbb7a692b996ddb843"}},"757442692f374d83b50fd1c65464aa80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bdd4db22bd143739734dd8869a3d774","placeholder":"â€‹","style":"IPY_MODEL_5da28a954549474bbfbf776c043365b5","value":"Map:â€‡100%"}},"af35a654bf9244bbaef897af30bd1314":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_455944f2fbb14265b39c9dde30f149b1","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db35d08399ec4efa9ef6bbcf81654b88","value":5}},"68cbe962fc3e4ea88756cc9d339cabe2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91de4a42a8a14cb78e8f21d6e55bd820","placeholder":"â€‹","style":"IPY_MODEL_522075398f6441d29e99c05deff49914","value":"â€‡5/5â€‡[00:00&lt;00:00,â€‡141.93â€‡examples/s]"}},"0ef59309dccb46cbb7a692b996ddb843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bdd4db22bd143739734dd8869a3d774":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5da28a954549474bbfbf776c043365b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"455944f2fbb14265b39c9dde30f149b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db35d08399ec4efa9ef6bbcf81654b88":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"91de4a42a8a14cb78e8f21d6e55bd820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"522075398f6441d29e99c05deff49914":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# ============================================\n","# ðŸŒŸ GPT-2 Text Generation Project\n","# Prodigy Infotech Internship Task (Final Version)\n","# By: [Your Name]\n","# ============================================\n","\n","# ðŸ§© STEP 1 â€” Install all dependencies (fixed version)\n","!pip install -U pip setuptools wheel\n","!pip install tokenizers==0.13.3 --only-binary=:all:\n","!pip install transformers==4.31.0 datasets torch --quiet\n","\n","# ðŸ§© STEP 2 â€” Import all libraries\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"  # disable Weights & Biases pop-up\n","\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from datasets import Dataset\n","import torch\n","\n","# ðŸ§  STEP 3 â€” Load GPT-2 model & tokenizer\n","model_name = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Fix padding token issue\n","tokenizer.pad_token = tokenizer.eos_token\n","model.config.pad_token_id = model.config.eos_token_id\n","\n","# ðŸ“ STEP 4 â€” Create your small dataset\n","# You can replace these lines with your own text data\n","text_data = [\n","    \"Artificial intelligence is changing the world of technology.\",\n","    \"Machine learning helps computers learn from experience.\",\n","    \"Natural language processing enables communication with computers.\",\n","    \"Data science combines math and coding to solve real problems.\",\n","    \"AI will make automation smarter and more efficient in the future.\"\n","]\n","\n","dataset = Dataset.from_dict({\"text\": text_data})\n","\n","# Tokenize dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n","\n","tokenized_datasets = dataset.map(tokenize_function, batched=True)\n","train_dataset = tokenized_datasets  # using all data for training\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n","# âš™ï¸ STEP 5 â€” Set training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=2,\n","    per_device_train_batch_size=2,\n","    save_steps=500,\n","    save_total_limit=2,\n","    logging_steps=5,\n",")\n","\n","# ðŸ‹ï¸â€â™‚ï¸ STEP 6 â€” Train the model\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    data_collator=data_collator,\n",")\n","\n","print(\"ðŸš€ Training started... Please wait.\")\n","trainer.train()\n","\n","# âœ¨ STEP 7 â€” Generate text\n","prompt = \"Artificial intelligence\"\n","inputs = tokenizer(prompt, return_tensors=\"pt\")\n","\n","outputs = model.generate(\n","    **inputs,\n","    max_length=80,\n","    temperature=0.7,\n","    top_p=0.9,\n","    do_sample=True\n",")\n","\n","print(\"\\nðŸ§  Generated Text:\\n\")\n","print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","\n","# ðŸ’¾ STEP 8 â€” Save model\n","model.save_pretrained(\"./fine_tuned_model\")\n","tokenizer.save_pretrained(\"./fine_tuned_model\")\n","\n","print(\"\\nâœ… Training complete! Model saved in 'fine_tuned_model' folder.\")\n","print(\"ðŸŽ‰ You can now use it to generate custom text!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":975,"referenced_widgets":["5849aea4f9b640349f56d43437813166","757442692f374d83b50fd1c65464aa80","af35a654bf9244bbaef897af30bd1314","68cbe962fc3e4ea88756cc9d339cabe2","0ef59309dccb46cbb7a692b996ddb843","3bdd4db22bd143739734dd8869a3d774","5da28a954549474bbfbf776c043365b5","455944f2fbb14265b39c9dde30f149b1","db35d08399ec4efa9ef6bbcf81654b88","91de4a42a8a14cb78e8f21d6e55bd820","522075398f6441d29e99c05deff49914"]},"id":"OgDR34Ix_NC_","executionInfo":{"status":"ok","timestamp":1759846092190,"user_tz":-330,"elapsed":155555,"user":{"displayName":"Pranay Phepade","userId":"14731711711372882303"}},"outputId":"c6e6d345-03f5-4046-8695-74cf24d5c3f3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n","\u001b[31mERROR: Ignored the following yanked versions: 0.20.4\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tokenizers==0.13.3 (from versions: 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.19.0, 0.19.1, 0.20.0, 0.20.1rc1, 0.20.1, 0.20.2, 0.20.3rc0, 0.20.3, 0.20.4rc0, 0.21.0rc0, 0.21.0, 0.21.1rc0, 0.21.1, 0.21.2rc0, 0.21.2, 0.21.4, 0.22.0, 0.22.1rc0, 0.22.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tokenizers==0.13.3\u001b[0m\u001b[31m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31mÃ—\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0m\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n","\n","\u001b[31mÃ—\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n","\u001b[31mâ•°â”€>\u001b[0m tokenizers\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/5 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5849aea4f9b640349f56d43437813166"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]},{"output_type":"stream","name":"stdout","text":["ðŸš€ Training started... Please wait.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n","  warnings.warn(warn_msg)\n","`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6/6 00:24, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>5</td>\n","      <td>3.636600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","ðŸ§  Generated Text:\n","\n","Artificial intelligence and robotics is already in its infancy, and that is not likely to change.\n","\n","\"It will make a lot of difference in the future. It's not going to be perfect. It's going to be better than what we've done,\" said Rene Fournier, an analyst at the University of Michigan's Hoover Institution in Palo Alto, California.\n","\n","The goal\n","\n","âœ… Training complete! Model saved in 'fine_tuned_model' folder.\n","ðŸŽ‰ You can now use it to generate custom text!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"holyDqwLIb1T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4YqAdz9y_esZ"},"execution_count":null,"outputs":[]}]}